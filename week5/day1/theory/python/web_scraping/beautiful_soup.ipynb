{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_name.contents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "\n",
    "- Beautiful soup\n",
    "\n",
    "- Selenium: Se usa más para interactuar con la WEB (también obtiene información)\n",
    "\n",
    "Example Selenium:\n",
    "\n",
    "https://towardsdatascience.com/web-scraping-using-selenium-python-8a60f4cf40ab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To install:**\n",
    "\n",
    "        pip3 install beautifulsoup4 requests pandas\n",
    "\n",
    "        pip3 install beautifulsoup4 \n",
    "\n",
    "        pip3 install requests\n",
    "\n",
    "        pip3 install pandas "
   ]
  },
  {
   "source": [
    "### Steps:\n",
    "\n",
    "1. Find the URL that you want to scrape.\n",
    "2. Inspecting the Page.\n",
    "3. Find the data you want to extract.\n",
    "4. Write the code.\n",
    "5. Run the code and extract the data.\n",
    "6. Store the data in the required format"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples Beautiful Soup:\n",
    "\n",
    "https://j2logo.com/python/web-scraping-con-python-guia-inicio-beautifulsoup/\n",
    "\n",
    "http://omz-software.com/pythonista/docs/ios/beautifulsoup_guide.html\n",
    "\n",
    "https://towardsdatascience.com/top-5-beautiful-soup-functions-7bfe5a693482\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show beautiful html\n",
    "def show_html(html_str):\n",
    "    print(BeautifulSoup(str(html_str), 'html.parser').prettify())"
   ]
  },
  {
   "source": [
    "** Your Headers ** \n",
    "\n",
    "http://www.xhaus.com/headers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"\"\"https://www.imdb.com/search/title/?count=100&groups=top_1000&sort=user_rating\"\"\"\n",
    "\n",
    "def get_page_contents(url):\n",
    "    page = requests.get(url, headers={\"Accept-Language\": \"en-US\"})\n",
    "    return BeautifulSoup(page.text, \"html.parser\")\n",
    "soup = get_page_contents(url)\n",
    "show_html(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Página de prueba</title>\n",
    "</head>\n",
    "<body>\n",
    "<div id=\"main\" class=\"full-width\">\n",
    "    <h1>El título de la página</h1>\n",
    "    <p>Este es el primer párrafo</p>\n",
    "    <p>Este es el segundo párrafo</p>\n",
    "    <div id=\"innerDiv\">\n",
    "        <div class=\"links\">\n",
    "            <a href=\"https://pagina1.xyz/\">Enlace 1</a>\n",
    "            <a href=\"https://pagina2.xyz/\">Enlace 2</a>\n",
    "        </div>\n",
    "        <div class=\"right\">\n",
    "            <div class=\"links\">\n",
    "                <a href=\"https://pagina3.xyz/\">Enlace 3</a>\n",
    "                <a href=\"https://pagina4.xyz/\">Enlace 4</a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div id=\"footer\">\n",
    "        <!-- El footer -->\n",
    "        <p>Este párrafo está en el footer</p>\n",
    "        <div class=\"links footer-links\">\n",
    "            <a href=\"https://pagina5.xyz/\">Enlace 5</a>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example artists: \n",
    "\n",
    "https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Collect first page of artists’ list\n",
    "page = requests.get('https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ1.htm')\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "show_html(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pull all text from the BodyText div\n",
    "artist_name_list = soup.find(class_='BodyText')\n",
    "show_html(artist_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pull text from all instances of <a> tag within BodyText div\n",
    "artist_name_list_items = artist_name_list.find_all('a')\n",
    "show_html(artist_name_list_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create for loop to print out all artists\n",
    "for artist_name in artist_name_list_items:\n",
    "    print(artist_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artist_names = []\n",
    "\n",
    "# Create for loop to print out all artists' names\n",
    "for artist_name in artist_name_list_items:\n",
    "    names = artist_name.contents[0]\n",
    "    print(names)\n",
    "    artist_names.append(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create for loop to print out all artists' links\n",
    "for artist_name in artist_name_list_items:\n",
    "    link = artist_name.get('href')\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "# Create for loop to print out all artists' links well\n",
    "for artist_name in artist_name_list_items:\n",
    "    link = 'https://web.archive.org' + artist_name.get('href')\n",
    "    print(link)\n",
    "    links.append(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a dict? \n",
    "\n",
    "artist_names_links = dict(zip(artist_names, links))\n",
    "artist_names_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a diccionario para trabajar con DataFrame?\n",
    "\n",
    "dict_df = {\"name\": artist_names, \"link\": links}\n",
    "df = pd.DataFrame(dict_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funny bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "webbrowser.open_new(\"www.google.es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_virus(urls):\n",
    "    for _ in urls:\n",
    "        webbrowser.open_new(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :D \n",
    "mini_virus(urls=links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example wikipedia Links:\n",
    "\n",
    "**Create a list of links on that page**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url ='https://en.wikipedia.org/wiki/Python'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "show_html(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get all \"a\"\n",
    "\n",
    "links = soup.findAll('a')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_links = []\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_links = []\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    if href and href[:4] == \"http\":\n",
    "        print(href)\n",
    "        list_of_links.append(href)"
   ]
  },
  {
   "source": [
    "### Top ten criminals\n",
    "\n",
    "#### Get and save images locally"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of images\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "\n",
    "imgs = []\n",
    "\n",
    "res= requests.get(url)\n",
    "html=res.text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "fbi = soup.select('img')\n",
    "fbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in fbi:\n",
    "    imgs.append(x.get(\"src\"))\n",
    "imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little bonus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, img_url in enumerate(imgs):\n",
    "    if not \"+\" in img_url:  # or http in img_url\n",
    "        response = requests.get(img_url)\n",
    "        if response.status_code == 200:  # Get OK\n",
    "            with open(str(i) + \".png\", 'wb') as f:\n",
    "                f.write(response.content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5c4d2f1fdcd3716c7a5eea90ad07be30193490dd4e63617705244f5fd89ea793"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}